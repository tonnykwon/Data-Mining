{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, udf, explode, monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"SPADE\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# create context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|hoagie institutio...|\n",
      "|excellent food su...|\n",
      "|yes place little ...|\n",
      "|food great best t...|\n",
      "|checked place pas...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_data = spark.read.text('reviews_sample.txt')\n",
    "review_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|               value|sid|\n",
      "+--------------------+---+\n",
      "|[hoagie, institut...|  0|\n",
      "|[excellent, food,...|  1|\n",
      "|[yes, place, litt...|  2|\n",
      "|[food, great, bes...|  3|\n",
      "|[checked, place, ...|  4|\n",
      "|[wing, sauce, lik...|  5|\n",
      "|[cold, cheap, bee...|  6|\n",
      "|[highly, recommen...|  7|\n",
      "|[big, believer, f...|  8|\n",
      "|[decent, range, s...|  9|\n",
      "|[owning, driving,...| 10|\n",
      "|[place, absolute,...| 11|\n",
      "|[finally, made, r...| 12|\n",
      "|[drove, yesterday...| 13|\n",
      "|[thank, rob, trul...| 14|\n",
      "|[waiting, almost,...| 15|\n",
      "|[visited, store, ...| 16|\n",
      "|[fianc, upgraded,...| 17|\n",
      "|[waited, min, peo...| 18|\n",
      "|[place, delicious...| 19|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split into list of words and add sid column\n",
    "\n",
    "review_split = review_data.withColumn('value', split(review_data.value, ' '))\\\n",
    ".select('value', monotonically_increasing_id().alias('sid'))\n",
    "review_split.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-----------+\n",
      "|               value|sid|       word|\n",
      "+--------------------+---+-----------+\n",
      "|[hoagie, institut...|  0|     hoagie|\n",
      "|[hoagie, institut...|  0|institution|\n",
      "|[hoagie, institut...|  0|    walking|\n",
      "|[hoagie, institut...|  0|        doe|\n",
      "|[hoagie, institut...|  0|       seem|\n",
      "|[hoagie, institut...|  0|       like|\n",
      "|[hoagie, institut...|  0|  throwback|\n",
      "|[hoagie, institut...|  0|       year|\n",
      "|[hoagie, institut...|  0|        ago|\n",
      "|[hoagie, institut...|  0|        old|\n",
      "|[hoagie, institut...|  0|  fashioned|\n",
      "|[hoagie, institut...|  0|       menu|\n",
      "|[hoagie, institut...|  0|      board|\n",
      "|[hoagie, institut...|  0|      booth|\n",
      "|[hoagie, institut...|  0|      large|\n",
      "|[hoagie, institut...|  0|  selection|\n",
      "|[hoagie, institut...|  0|       food|\n",
      "|[hoagie, institut...|  0| speciality|\n",
      "|[hoagie, institut...|  0|    italian|\n",
      "|[hoagie, institut...|  0|     hoagie|\n",
      "+--------------------+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create sid index\n",
    "\n",
    "cols = ('value', 'sid')\n",
    "review_explode = review_split.select(*cols, explode('value').alias('word') )\n",
    "review_explode.select(*tuple(review_explode.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+\n",
      "|value| sid|      word|\n",
      "+-----+----+----------+\n",
      "|    1|5776|  adequate|\n",
      "|    2|5776|      also|\n",
      "|    3|5776|      area|\n",
      "|    4|5776|       ask|\n",
      "|    5|5776|      back|\n",
      "|    6|5776| beautiful|\n",
      "|    7|5776|      best|\n",
      "|    8|5776|      best|\n",
      "|    9|5776|      best|\n",
      "|   10|5776|    blonde|\n",
      "|   11|5776|    brassy|\n",
      "|   12|5776|    bumble|\n",
      "|   13|5776|    bumble|\n",
      "|   14|5776|      came|\n",
      "|   15|5776|      came|\n",
      "|   16|5776|     could|\n",
      "|   17|5776|  customer|\n",
      "|   18|5776|     decor|\n",
      "|   19|5776|definetely|\n",
      "|   20|5776|definetely|\n",
      "+-----+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create eid by using window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql import Window\n",
    "\n",
    "w = Window.partitionBy(\"value\").orderBy(\"word\")\n",
    "review_enumerate = review_explode.withColumn(\"value\", row_number().over(w))\n",
    "review_enumerate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['adequate', 'also', 'area', 'ask', 'back', 'beautiful', 'best',\n",
       "       'best', 'best', 'blonde', 'brassy', 'bumble', 'bumble', 'came',\n",
       "       'came', 'could', 'customer', 'decor', 'definetely', 'definetely',\n",
       "       'desired', 'desk', 'experience', 'experience', 'experience',\n",
       "       'fixed', 'front', 'good', 'good', 'great', 'hair', 'handled',\n",
       "       'high', 'instead', 'jim', 'job', 'kerastase', 'large', 'league',\n",
       "       'like', 'like', 'little', 'mixed', 'much', 'new', 'new',\n",
       "       'nicolette', 'one', 'orange', 'others', 'overall', 'owner', 'par',\n",
       "       'people', 'people', 'poorly', 'price', 'product', 'push', 'result',\n",
       "       'service', 'something', 'space', 'sub', 'thing', 'think',\n",
       "       'treatment', 'tried', 'value', 'year', 'york'], dtype='<U10')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "data = pd.read_table('reviews_sample.txt', header=None)\n",
    "np.sort(data[0][5776].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid udf: the udf argument must be a pandas_udf of type GROUPED_MAP.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-0007026d88a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mto_list_udf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mreview_enumerate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_list_udf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-2.4.0-bin-hadoop2.7\\python\\pyspark\\sql\\group.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, udf)\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mudf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mudf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'func'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m            \u001b[1;32mor\u001b[0m \u001b[0mudf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevalType\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mPythonEvalType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSQL_GROUPED_MAP_PANDAS_UDF\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m             raise ValueError(\"Invalid udf: the udf argument must be a pandas_udf of type \"\n\u001b[0m\u001b[0;32m    272\u001b[0m                              \"GROUPED_MAP.\")\n\u001b[0;32m    273\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid udf: the udf argument must be a pandas_udf of type GROUPED_MAP."
     ]
    }
   ],
   "source": [
    "# group by word and cut length\n",
    "from pyspark.sql.functions import countDistinct, lit\n",
    "\n",
    "def to_list(x,y):\n",
    "    return list(x,y)\n",
    "\n",
    "to_list_udf = udf(to_list)\n",
    "\n",
    "review_enumerate.groupby('word').apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
