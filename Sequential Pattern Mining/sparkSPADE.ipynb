{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, udf, explode, monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['SPARK_HOME'] = \"C:\\spark-2.4.0-bin-hadoop2.7\"\n",
    "# os.environ['JAVA_HOME'] = \"C:\\Program Files\\Java\\jdk-9.0.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"SPADE\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# create context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "inside_udf = udf(inside)\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read\n",
    "review_data = spark.read.text('reviews_sample.txt')\n",
    "min_sup = review_data.count()*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into list of words and add sid column\n",
    "\n",
    "review_split = review_data.withColumn('value', split(review_data.value, ' '))\\\n",
    ".select('value', monotonically_increasing_id().alias('sid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process 1 length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sid index\n",
    "\n",
    "cols = ('value', 'sid')\n",
    "review_explode = review_split.select(*cols, explode('value').alias('word') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+\n",
      "|value| sid|      word|\n",
      "+-----+----+----------+\n",
      "|    1|5776|  adequate|\n",
      "|    2|5776|      also|\n",
      "|    3|5776|      area|\n",
      "|    4|5776|       ask|\n",
      "|    5|5776|      back|\n",
      "|    6|5776| beautiful|\n",
      "|    7|5776|      best|\n",
      "|    8|5776|      best|\n",
      "|    9|5776|      best|\n",
      "|   10|5776|    blonde|\n",
      "|   11|5776|    brassy|\n",
      "|   12|5776|    bumble|\n",
      "|   13|5776|    bumble|\n",
      "|   14|5776|      came|\n",
      "|   15|5776|      came|\n",
      "|   16|5776|     could|\n",
      "|   17|5776|  customer|\n",
      "|   18|5776|     decor|\n",
      "|   19|5776|definetely|\n",
      "|   20|5776|definetely|\n",
      "+-----+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create eid by using window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql import Window\n",
    "\n",
    "w = Window.partitionBy(\"value\").orderBy(\"word\")\n",
    "review_enumerate = review_explode.withColumn(\"value\", row_number().over(w))\n",
    "review_enumerate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# group by word and cut length\n",
    "from pyspark.sql.functions import collect_list, struct, col, size\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "\n",
    "def distinct_size(x):\n",
    "    return len(pd.unique(x))\n",
    "\n",
    "def zip_spark(x, y):\n",
    "    return list(zip(x,y))\n",
    "\n",
    "zip_spark_udf = udf(zip_spark, returnType=ArrayType(ArrayType(IntegerType(), IntegerType())))\n",
    "\n",
    "distinct_size_udf = udf(distinct_size, returnType=IntegerType())\n",
    "\n",
    "# combine sid, value(eid) and group based on word\n",
    "review_zip = review_enumerate.groupby('word').agg(collect_list('sid').alias('sid'), collect_list('value').alias('eid'))\\\n",
    ".where(distinct_size_udf('sid') >=min_sup) # zip columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+\n",
      "|        word|                 sid|                 eid|\n",
      "+------------+--------------------+--------------------+\n",
      "|        hope|[1617, 2761, 7627...|[58, 13, 54, 33, ...|\n",
      "|       still|[1613, 1613, 9600...|[144, 145, 66, 67...|\n",
      "|         art|[2699, 6140, 2988...|[8, 4, 4, 5, 6, 7...|\n",
      "|      online|[723, 723, 4390, ...|[72, 73, 14, 11, ...|\n",
      "|       crust|[4266, 4284, 2732...|[15, 30, 6, 21, 1...|\n",
      "|      filled|[943, 9857, 1931,...|[35, 81, 22, 23, ...|\n",
      "|    received|[7215, 6306, 631,...|[47, 51, 37, 95, ...|\n",
      "|    slightly|[1903, 6475, 470,...|[43, 34, 26, 27, ...|\n",
      "|       staff|[5832, 3294, 7884...|[132, 75, 78, 79,...|\n",
      "|conversation|[6743, 2424, 9568...|[28, 21, 15, 7, 4...|\n",
      "|       often|[9958, 2699, 2699...|[15, 118, 119, 74...|\n",
      "|    positive|[4003, 1651, 50, ...|[15, 45, 88, 56, ...|\n",
      "|      taking|[9857, 8732, 3809...|[166, 80, 113, 39...|\n",
      "|      worked|[1075, 4032, 4413...|[125, 50, 101, 40...|\n",
      "|       watch|[810, 7807, 1863,...|[21, 31, 44, 90, ...|\n",
      "|        cook|[4284, 4284, 6743...|[24, 25, 30, 7, 9...|\n",
      "|        slow|[6820, 3234, 7078...|[44, 15, 82, 30, ...|\n",
      "|        dive|[3491, 109, 4716,...|[4, 21, 24, 44, 6...|\n",
      "|       leave|[1451, 6810, 7078...|[33, 25, 40, 13, ...|\n",
      "|       ready|[8764, 5832, 3849...|[68, 120, 33, 54,...|\n",
      "+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "review_zip.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "data = pd.read_table('reviews_sample.txt', header=None)\n",
    "np.sort(data[0][5776].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_zip = review_enumerate.select('word', struct('sid', 'value').alias('id'))\\\n",
    ".groupby('word').agg(collect_list('id').alias('id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
