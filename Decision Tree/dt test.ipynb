{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read train data\n",
    "data = pd.read_table('training.txt', sep=' ', header=None)\n",
    "# name label\n",
    "data.rename(columns={0:'label'}, inplace=True)\n",
    "# sepearte labels\n",
    "label = data.iloc[:, 0]\n",
    "\n",
    "# remove index: from data\n",
    "data = data.iloc[:, 1:]\n",
    "data = data.iloc[:, 1:].applymap(lambda x: x[x.find(':')+1:])\n",
    "\n",
    "# rename columns and merge label\n",
    "data.columns = list(range(data.shape[1]))\n",
    "data = data.join(label)\n",
    "# label: int, other columns: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read test data\n",
    "test = pd.read_table('testing.txt', sep=' ', header=None)\n",
    "\n",
    "# remove index: from data\n",
    "test = test.applymap(lambda x: x[x.find(':')+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "n = data.shape[0]\n",
    "idx = np.random.randint(0, n, int(n/20))\n",
    "val_data = data.iloc[idx,:]\n",
    "train_data = data.drop(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gini(data):\n",
    "    n = data.shape[0]\n",
    "    label_count = data.groupby('label')['label'].count()\n",
    "    prob = label_count/n\n",
    "\n",
    "    return 1-np.sum(np.square(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(data, column, feature):\n",
    "    left = data[column][data[column]==feature].index.values\n",
    "    right = data[column][data[column]!=feature].index.values\n",
    "    \n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(data):\n",
    "    label_count = data.groupby('label')['label'].count()\n",
    "    \n",
    "    return label_count.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data indexing\n",
    "train_data.index = range(0, train_data.shape[0])\n",
    "val_data.index = range(0, val_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Build Tree\n",
    "max_depth = 20\n",
    "threshold = 0.005\n",
    "min_node = 20\n",
    "min_leaf = 10\n",
    "\n",
    "# data copy\n",
    "data_copy = train_data.copy()\n",
    "\n",
    "## setup\n",
    "depth = 0\n",
    "result_list = []\n",
    "search_list = [(data_copy.index.values, depth)]\n",
    "base_gini_list = [get_gini(data_copy)]\n",
    "\n",
    "# breadth first search\n",
    "while search_list:\n",
    "    \n",
    "    node, depth = search_list.pop(0)\n",
    "    base_gini = base_gini_list.pop(0)\n",
    "    train = data_copy.loc[node]\n",
    "    gini_list = []\n",
    "    \n",
    "    # case used all columns\n",
    "    if train.shape[1] <=1:\n",
    "        break\n",
    "\n",
    "    # case when depth is higher than max_depth\n",
    "    if depth >= max_depth:\n",
    "        result_list.append((None, None, None))\n",
    "        continue\n",
    "    \n",
    "    n = len(node)\n",
    "    # case when node is too small to split\n",
    "    if n <= min_node:\n",
    "        result_list.append((None, None, None))\n",
    "        continue\n",
    "\n",
    "    # loop over columns except labels\n",
    "    for column in train.iloc[:,:-1]:\n",
    "        for feature in np.unique(train[column]):\n",
    "            left_idx, right_idx = data_split(train, column, feature)\n",
    "\n",
    "            # check if splits are smaller than min_leaf\n",
    "            left_n = len(left_idx)\n",
    "            right_n = len(right_idx)\n",
    "            if left_n > min_leaf and right_n > min_leaf:\n",
    "                left_ratio = left_n/n\n",
    "                right_ratio = right_n/n\n",
    "                gini = left_ratio*get_gini(train.loc[left_idx])+right_ratio*get_gini(train.loc[right_idx])\n",
    "                gini_list.append((column, feature, gini))\n",
    "    \n",
    "    # find minimum gini\n",
    "    if len(gini_list) <1:\n",
    "        result_list.append((None, None, None))\n",
    "        continue\n",
    "    column, feature, min_gini = min(gini_list, key= lambda x: x[2])\n",
    "    #print('min gini: '+ str(min_gini))\n",
    "    \n",
    "    # check if satisfies threshold\n",
    "    if base_gini-min_gini < threshold:\n",
    "        result_list.append((None, None, None))\n",
    "        continue\n",
    "    \n",
    "    # remove used column\n",
    "    left_idx, right_idx = data_split(train, column, feature)\n",
    "    data_copy = data_copy.drop(columns=column)\n",
    "    \n",
    "    # append left and right node\n",
    "    depth +=1\n",
    "    result_list.append((column, feature, depth))\n",
    "    search_list.append((left_idx, depth))\n",
    "    search_list.append((right_idx, depth))\n",
    "    \n",
    "    # update base_gini and appends left, right base_gini\n",
    "    base_gini = min_gini\n",
    "    base_gini_list.append(base_gini)\n",
    "    base_gini_list.append(base_gini)\n",
    "    \n",
    "    # print((column, feature, depth))\n",
    "    # print('left num: '+str(len(left_idx)))\n",
    "    # print('right num: '+str(len(right_idx)))\n",
    "    # print('new base gini: '+str(base_gini) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## predict\n",
    "# copy test set\n",
    "test_set = val_data.copy()\n",
    "train_set = train_data.copy()\n",
    "\n",
    "# setup\n",
    "split_list = result_list.copy()\n",
    "train_set['predict'] = None\n",
    "test_set['predict'] = None\n",
    "train_list = [train_set.index]\n",
    "test_list = [test_set.index]\n",
    "\n",
    "while split_list:\n",
    "    # get split criteria and datasets from each\n",
    "    column, feature, depth = split_list.pop(0)\n",
    "    \n",
    "    #print(str(column)+' '+str(feature)+', '+str(depth))\n",
    "    train_idx = train_list.pop(0)\n",
    "    test_idx = test_list.pop(0)\n",
    "    train = train_set.loc[train_idx]\n",
    "    test = test_set.loc[test_idx]\n",
    "    \n",
    "    # certain condition not met\n",
    "    if column is None:\n",
    "        continue\n",
    "    \n",
    "    n = len(train_idx)\n",
    "    if n <= min_node:\n",
    "        continue\n",
    "    \n",
    "    train_left_idx, train_right_idx = data_split(train, column, feature)\n",
    "    test_left_idx, test_right_idx = data_split(test, column, feature)\n",
    "    \n",
    "    # set label\n",
    "    left_label = get_label(train_set.loc[train_left_idx])\n",
    "    right_label = get_label(train_set.loc[train_right_idx])\n",
    "    # test set labels\n",
    "    test_set.loc[test_left_idx, 'predict'] = left_label\n",
    "    test_set.loc[test_right_idx, 'predict'] = right_label\n",
    "    # train set labels\n",
    "    train_set.loc[train_left_idx, 'predict'] = left_label\n",
    "    train_set.loc[train_right_idx, 'predict'] = right_label\n",
    "\n",
    "    # put on the list\n",
    "    if len(train_left_idx) > min_leaf and len(train_right_idx) > min_leaf:\n",
    "        train_list.append(train_left_idx)\n",
    "        train_list.append(train_right_idx)\n",
    "        test_list.append(test_left_idx)\n",
    "        test_list.append(test_right_idx)\n",
    "        # print('left label: '+str(left_label))\n",
    "        # print('left num: '+str(len(train_left_idx)))\n",
    "        # print('right label: '+str(right_label))\n",
    "        # print('right num: '+str(len(train_right_idx)) +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4], dtype=int64), array([81, 17, 16, 36], dtype=int64))"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_set.predict, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4640477025605051"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_set.label == train_set.predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
